<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ChK</title>
    <description>我的个人博客</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 21 May 2019 15:33:51 +0800</pubDate>
    <lastBuildDate>Tue, 21 May 2019 15:33:51 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Chapter2. Multi-armed Bandits</title>
        <description>&lt;p&gt;&lt;strong&gt;区分增强学习与其它机器学习方法的最重要特征&lt;/strong&gt;：用 评价动作选择的训练信息(用 policy 来选择动作，增强学习会评价所选择的动作的好坏) 来取代 直接给出正确动作的指导信息 (由外部直接给定正确的动作)	。为了明确搜索好的行为，增强学习需要积极地探索。单纯的评价性反馈支出采取的动作有多好，而不是说哪个动作最好或哪个最差；而单纯的指导性反馈恰恰相反，其指出应该采取的正确动作。在这种单纯的形式上，这两种反馈有明显不同，评价性反馈完全依赖于动作的选取，而指导性反馈则完全独立于动作的选取。&lt;/p&gt;

&lt;p&gt;这一章在一种简单的环境下(不超过一种situation)学习评价方面的RL，这种无关联环境中，评价性反馈所涉及的前置工作已经完成，避免了完整的RL问题中的许多复杂情况。学习这个例子，能够清楚地了解评价性反馈是如何区别于指导性反馈，以及如何与其相结合。&lt;/p&gt;

&lt;p&gt;这章使用的无关联、评价性反馈问题是 &lt;em&gt;k-armed bandit proble&lt;/em&gt; 的一种简单版本，用它来引入一系列基本学习方法，并在后续章节中扩展他们以应用于完整的RL问题。在本章的最后，通过讨论 bandit 问题编程 associative ，即 situation 多于一种的情况下的RL问题。&lt;/p&gt;

&lt;h3 id=&quot;21-a-k-armed-bandit-problem&quot;&gt;2.1 A k-armed Bandit Problem&lt;/h3&gt;
&lt;p&gt;所谓  Bandit Problem ，就是指要在一个动作空间中做出选择，然后该选择将会决定获得的奖励的多少，在不停的做选择并得到奖励回馈中不断优化选择策略。&lt;/p&gt;

&lt;p&gt;此处的问题中，需要在k个不同选项中重复地做选择，每次选择后会得到一个数值奖励，其出自一个依赖于所做选择的统计概率分布，目标是在经过一段时间(如1000次选择或一段实际的时间)让总奖励的期望最大化。&lt;/p&gt;

&lt;p&gt;在该问题中，每个 action  都有一个期望或平均奖励，称为该  action  的  value 。将在 time step  $t$ 做的选择定为 $A_t$，其对应的奖励值为 $R_t$，对任一 action  $a$，定义 $q_\ast (a)$ 为 $a$ 的期望奖励:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_\ast (a)\doteq \Bbb E[R_t|A_t=a].&lt;/script&gt;

&lt;p&gt;由于$q_\ast (a)$ 不完全可知，定义  estimated value of action  为 $Q_t(a)$，目标是让$Q_t(a)$ 靠近 $q_\ast (a)$
这里的$q_\ast (a)$ 的定义是针对 Bandit Problem  的，每次的选择不会影响到后面的情况，因此只要考虑即时 reward&lt;/p&gt;

&lt;p&gt;当我们保存下这些  estimates of the action values ，在某些 time step 中，我们能够找到 value  最高的 action ，它们称为  greedy action ，如果我们选择这些action，就叫做 exploiting ；反之，当我们选择那些不是 greedy actions  的 action 时，称作  exploring ，这能优化那些  non-greedy action  的  value ，这主要依赖于后续的未知变化。&lt;/p&gt;

&lt;p&gt;在任意确定案例中， explore  和  exploit  哪个好是一件很复杂的事情，其取决于估计的精确值，不确定性及剩余步数。在 bandit problems 和其它相关问题中，能够用特定的数学表达来得到许多精妙的方法来平衡这两者，但是这些方法大都对平稳和先验做了强假设，它们都不能实际应用在完全的RL问题中，当这些理论假设不成立时，这些方法的优化和有限损失的保证均不靠谱。&lt;/p&gt;

&lt;h3 id=&quot;22-action-value-methods&quot;&gt;2.2 Action-value Methods&lt;/h3&gt;
&lt;p&gt;对 action-value  的估计可以简单地表示为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_t(a)\doteq \frac
{\text{sum of rewards when $a$ taken prior to $t$ } }
{\text {number of times $a$ taken prior to $t$ } }
=\frac
{\sum_{i=1}^{t-1}R_i\cdot I_{A_i=a} }
{\sum_{i=1}^{t-1}I_{A_i=a} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
I_{A_i=a}\doteq
\begin{cases}
1, &amp; if \ A_i=a \\
0, &amp; if \ A_i\neq a
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;分子为在 $t$ 时刻前所有选择 action  $a$ 所获得的 rewards  之和，分母为在 $t$ 时刻前选择 action  $a$ 的总次数。&lt;/p&gt;

&lt;p&gt;$Q_t(a)$ 表示选择一次  action  $a$ 所获得的平均 reward 。特别地，当分母为0时，可以定义$Q_t(a)$为0.&lt;/p&gt;

&lt;p&gt;当分母趋于无穷时，根据大数定理，$Q_t(a)$ 趋于 $q_\ast (a)$ ，该方法称为  sample-average technique&lt;/p&gt;

&lt;p&gt;最简单的  action  选取规则就是选择拥有最高  estimated value  的那个  action ，当有多个  actions  拥有最高  estimated value  时，任意选择其中一个即可，即：
 $A_t\doteq \mathop{\arg\min}_aQ_t(a)$&lt;/p&gt;

&lt;p&gt;该方法是完全  exploit ，不进行 explore ，一个简单的改进是引入一个概率值 $\epsilon$，来使选取策略以一定概率进行 explore ，即不再选择拥有最高  estimated value  的  action ，而是随机选取 action ，这称为 ε-greedy methods 。该方法的优势在于，随着 steps  的增加，每个  action  总会有机会被选中，使得 $Q_t(a)$ 整体上趋于 $q_\ast (a)$&lt;/p&gt;

&lt;h4 id=&quot;exercise&quot;&gt;Exercise：&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;2.1&lt;/em&gt;：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$50\%$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;2.2：Bandit example&lt;/em&gt;：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A2、A5肯定是  ε case ，因为在A2以前， action 2  没有被选择过，其Q值为0，而 action 1 的Q值已经非0，当时的 greedy action 应该是 action 1 ，所以 action 2 肯定不是 greedy action ，A5同理；A1、A3有可能是 ε case ，在 time step 3 ， greedy action  应该是  action 1  以及  action 2  ，按照对 greedy action 的选取策略，如果是随机选中到了 action 1 ， 那么A3就是 ε case ，如果是选中了 action 2 ，那么A3就不是 ε case ，A1与其类似。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;23-the-10-armed-testbed&quot;&gt;2.3 The 10-armed Testbed&lt;/h3&gt;
&lt;p&gt;粗略评估 greedy 和 ε-greedy 的效率差异：&lt;/p&gt;

&lt;p&gt;10-armed testbed ：2000次随机生成的 10-armed bandit problem ，每一次都以标准高斯分布选取10个 $q_\ast (a)$，如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RL-Intro-Chapter2/10q*a.png&quot; alt=&quot;10qValue&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于每一个 $At = a(a = 1, 2,\dots,10)$ 其 reward 都从高斯分布中选取( mean  = $q_\ast (a)$，  variance  = 1)
对于方法的评估：对每一个  10-armed bandit problem  ，在经过 1000 time steps  后，称为一轮  run ，经过独立的2000轮次后(不同 10-armed bandit problem )，得到的结果作为该方法的性能评估&lt;/p&gt;

&lt;p&gt;结果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images//RL-Intro-Chapter2/diff-epsilon.png&quot; alt=&quot;diff-epsilon&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;这张图的初始Q值是直接设置为 0 的，如果用$Q_1(a)$ 作初值的话， optimal action 都会很快达到$80\%$ 左右&lt;/li&gt;
  &lt;li&gt;greedy method 在最开始时会略微地比 ε-greedy method 快一丢丢，但是很快收敛于低水平，其 average reward 约为1，其选中最优 action 的次数大约是三分之一，表示其得到了较次的策略
对于 ε-greedy method&lt;/li&gt;
  &lt;li&gt;$ε = 0.1$ 时，其进步较快，收敛也较快，最终 optimal action 选取率收敛于 $90\%$ 左右&lt;/li&gt;
  &lt;li&gt;$ε = 0.01$ 时，进步稍慢，收敛也稍慢，但最终表现会好过 $ε = 0.1$ 的情况，理论上其最终 optimal action 选取率应为 $99\%$，&lt;strong&gt;糟糕的是，图上并没有显示出 $ε = 0.01$ 最终的情况&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;ε-greedy method  取决于具体的任务情况，一般来说， reward  的方差大时， ε-greedy method 会更好，如果方差为0,那  ε-greedy method  就是在单纯地做无用功&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;exercise-1&quot;&gt;Exercise：&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;2.3&lt;/em&gt; ： $ε = 0.01$ 的情况会表现最好，假设在足够长的时间后，$ε = 0.1$ 和 $ε = 0.01$ 都找到了最优策略，那么前者会以$90\%$的概率选择 Optimal action ， 而后者会以$99\%$的概率选择 Optimal action ，那么显然后者带来的累计奖励更高。&lt;/p&gt;

&lt;h3 id=&quot;24-incremental-implementation&quot;&gt;2.4 Incremental Implementation&lt;/h3&gt;
&lt;p&gt;action-value methods 总是用 sample-average 来估计 values ，接下来讨论如何有效率地计算  average&lt;/p&gt;

&lt;p&gt;针对一个 action  $a$ ，令 $R_i$ 为第 $i$ 次选择 $a$ 所得的  reward ，令 $Q_n$ 为 选择 $n - 1$ 次 $a$ 后，对 $a$ 的 value 值的估计，则：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q_n\doteq \frac
{R_1+R_2+\cdots +R_{n-1}}
{n-1}
.&lt;/script&gt;

&lt;p&gt;显然，不能保存下每一个 $R_i$，每次都累加计算，时间空间效率都太低，均达到了$O(n)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Q_{n+1} &amp; = \frac 1n\sum_{i=1}^nR_i \\
&amp;= \frac 1n\Big(R_n+\sum_{i=1}^{n-1}R_i\Big) \\
&amp;= \frac 1n\Big(R_n+(n-1)\frac 1{n-1}\sum_{i=1}^{n-1}R_i\Big) \\
&amp;= \frac 1n\Big(R_n+(n-1)Q_n\Big) \\
&amp;= \frac 1n\Big(R_n+nQ_n-Q_n\Big) \\
&amp;= Q_n+\frac 1n\Big[R_n-Q_n\Big]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;显然，$Q_{n+1}$ 可以由 $Q_n$ 、$n$ 与 $R_n$ 三者计算得到 ，时间空间效率都能达到 $O(1)$ ；当然，也可以保存所有 $R$ 的累计和，但是该值会随时间增大，在 $t$ 没有极限的情况下，其有溢出的可能&lt;/p&gt;

&lt;p&gt;上式的一般形式如下：&lt;/p&gt;

&lt;p&gt;$NewEstimate \leftarrow OldEstimate + StepSize\ [\ Target - OldEstimate\ ]\ .$&lt;/p&gt;

&lt;p&gt;其中 $\ [\ Target - OldEstimate\ ]$ 视作估计误差，在估计值不断接近 $Target$ 的过程中随之减小，$StepSize$ 用于随  step  变化实现  Incremental Implementation  ，后面写作 $\alpha$&lt;/p&gt;

&lt;p&gt;最后，对于 bandit problem 的简单算法如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; Initialize, for\ a=1\ to\ k:  \\
&amp;\qquad  Q(a)\leftarrow 0\\
&amp;\qquad  N(a)\leftarrow 0 \\
&amp; Loop forever:   \\
&amp;\qquad A\leftarrow
\begin{cases} \arg\max_aQ(a) &amp; \text{with probability }1-\epsilon \ \ \text{  (breaking ties randomly)} \\
\text{a random action} &amp; \text{with probability }\epsilon
\end{cases}\\
&amp;\qquad R\leftarrow bandit(A)\\
&amp;\qquad N(A)\leftarrow N(A)+1\\
&amp;\qquad Q(A)\leftarrow Q(A)+\frac1{N(A)}\big[R-Q(A)\big]
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;25-tracking-a-nonstationary-problem&quot;&gt;2.5 Tracking a Nonstationary Problem&lt;/h3&gt;
&lt;p&gt;Nonstationary Problem ： reward probabilities  不是一成不变的，而是会随时间或其它量产生变化&lt;/p&gt;

&lt;p&gt;这种情况下，应该给最新的  reward  以高的权重，一般会改变 $\alpha$ 的值来做到这点： $Q_{n+1}\doteq Q_n+\alpha \Big[R_n-Q_n\Big]$ ，该 $\alpha$ 在前面的算法中，表现为 $1/n$ ，$\alpha \in (0, 1]$ ，$\alpha$ 越大，表示越看重新的 reward ；$\alpha$ 越小，则表明越看重之前已得到的估计。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Q_{n+1}&amp;=Q_n+\alpha \Big[R_N-Q_n\Big] \\
&amp; = \alpha R_n+(1-\alpha)Q_n  \\
&amp; = \alpha R_n+(1-\alpha)\big[\alpha R_{n-1}+(1-\alpha)Q_{n-1}\big] \\
&amp; = \alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2Q_{n-1} \\
&amp; = \alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2\alpha R_{n-2}+ \\
&amp;\qquad \qquad \qquad \cdots +(1-\alpha)^{n-1}\alpha R_1+(1-\alpha)^nQ_1  \\
&amp; = (1-\alpha)^nQ_1+\sum_{i=1}^n\alpha (1-\alpha)^{n-i}R_i.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;上式表明，$i$ 越小，也就是越靠前的 $R_i$，其权重越小，因为$1-\alpha&amp;lt;1$;
极端情况下，即$1 - \alpha = 0$，则上式写作：$Q_{n+1} = R_n$，
这种表达称作加权平均  weighted average ，因为有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1-\alpha)^n+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}=1, \qquad \text{(等比数列求和)}&lt;/script&gt;

&lt;p&gt;reward  的权重以 $1 - \alpha$ 的比例呈指数性衰减，这有时被称作  exponential recency-weighted average&lt;/p&gt;

&lt;p&gt;令 $\alpha_n(a)$ 为第 $n$ 次选择 action  $a$ 时所用的 step-size parameter 。在之前的学习中，$\alpha_n(a)= 1/n$ ，这能让$Q_n$ 收敛。&lt;/p&gt;

&lt;p&gt;但是很显然，并不是所有的 $\alpha_n(a)$ 都能让$Q_n$ 收敛，比如 $\alpha_n(a)=0.5$ 就不行。&lt;/p&gt;

&lt;p&gt;在&lt;strong&gt;随机逼近理论&lt;/strong&gt;(&lt;em&gt;stochastic approximation&lt;/em&gt;)中，给出了保证收敛的条件：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\sum_{n=1}^\infty \alpha_n(a)=\infty \qquad and \qquad \sum_{n=1}^\infty \alpha_n^2(a)&lt;\infty %]]&gt;&lt;/script&gt;

&lt;p&gt;前一个条件用于保证 steps 足够大，以排除任何初始条件或随机扰动的干扰；后一个条件保证了收敛性。&lt;/p&gt;

&lt;p&gt;在后面的案例中，第二个条件将得不到满足，这使得$Q_n$不会最终收敛于定值，而是始终受到$R_n$ 的影响，但正如之前所提到的，在非稳定的环境中，这正是我们所需要的。&lt;/p&gt;

&lt;p&gt;另外，满足两个条件的 $\alpha_n(a)$ 往往收敛得很慢，或者需要精心调整参数以得到一个令人满意的收敛速度。因此，这两个条件通常只用在理论工作中，在实际应用和研究中很少使用。&lt;/p&gt;

&lt;h4 id=&quot;exercise-2&quot;&gt;Exercise：&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;2.4&lt;/em&gt;：当 $\alpha$ 不为常数时，$Q_{n+1}$ 与 $R_1$~$R_n$的关系可表示如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Q_{n + 1} &amp; = Q_n + \alpha_n [R_n - Q_n] \\
 &amp; = (1 - \alpha_n)Q_n + \alpha_n R_n \\
 &amp; = (1 - \alpha_n)\Big((1 - \alpha_{n - 1})Q_{n-1} + \alpha_{n-1} R_{n-1}\Big) + \alpha_n R_n \\
 &amp; = (1 - \alpha_n)(1 - \alpha_{n - 1})Q_{n-1} + (1 - \alpha_n)\alpha_{n-1}R_{n-1} + \alpha_n R_n \\
 &amp; = Q_1 \prod_i^n(1-\alpha_i) + \sum_i^n \bigg(\alpha_i R_i \prod_{j=i+1}^{n} (1 - \alpha_j)\bigg)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;2.5：编程题&lt;/em&gt;：设计并实现一个实验，用来观察在非稳定环境下 sample-average methods  的缺点。&lt;/p&gt;

&lt;p&gt;使用 10-armed testbed 的变种：所有的初始$q_\ast (a)$ 都相等，并且每一步都分别为每一个 $q_\ast (a)$ 额外加上一个取自高斯分布的变化 ( $mean = 0\  and\ variance = 0.1$ )&lt;/p&gt;

&lt;p&gt;使用两个方法对比： sample-average  以及 $constant\ \alpha = 0.1$；设 $\epsilon = 0.1$；画出如 &lt;em&gt;2.2&lt;/em&gt; 的图像&lt;/p&gt;

&lt;p&gt;结果如下图，看得出来， sample-average methods  明显劣于&lt;em&gt;constant $\alpha$&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;这是因为 sample-average 在 $n$ 稍大后，其 $Q$ 函数便会很快收敛，而由于环境是 nonstationary ，所以收敛的 $Q$ 函数不适用于新的环境了&lt;/p&gt;

&lt;p&gt;而 &lt;em&gt;constant $\alpha$&lt;/em&gt; 中 ，新的  reward  会起到较重的作用，且 $Q$ 函数一般不收敛，所以能够跟着环境做调整&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images//RL-Intro-Chapter2/exercise2.5.png&quot; alt=&quot;exercise2.5&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;26-optimistic-initial-values&quot;&gt;2.6 Optimistic Initial Values&lt;/h3&gt;
&lt;p&gt;前面的方法中，初始化的方法为 $Q_1(a)$，即先为每一个 $a$ 运行一次，得到其 reward  作为初始 $Q$ 值，这会给 $Q$ 函数带来偏差 bias&lt;/p&gt;

&lt;p&gt;在 sample-average methods  中，这个偏差会很快消失；而在 constant  α 中，会随着时间减小&lt;/p&gt;

&lt;p&gt;而在实际应用中，这个偏差通常不会带来麻烦，反而在有些时候大有好处&lt;/p&gt;

&lt;p&gt;缺点是，该参数成了一组需要人为用心挑选的参数，即使只是全设成 $0$ ；而优点则是它能很方便地一些关于所期望的奖励水平的先验知识&lt;/p&gt;

&lt;p&gt;初始Q值也可以方便地用于鼓励 explore ，比如把上面的 10-armed bandit problems  的 $Q_1(a)$ 全部设成 $+5$ ，那么在开始的时候，算法总是会得到低于 $5$ 的 reward ，$Q$ 值被更新成较小的值，那么算法就会去尝试其它的 action ，而被选中的 action 的 $Q$ 值总是被减少，也就是说 greedy-action 会不停地变化；反复如此，便能轻松地起到鼓励 explore 的作用；而在$n$稍大一些时， $+5$ 的副作用便轻松的被消去了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images//RL-Intro-Chapter2/optimistic_initial_value.png&quot; alt=&quot;optimistic_initial_value&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这种鼓励 explore  的方法称作  optimistic initial values ， 在 stationary problems  可作为提高效率的小技巧&lt;/p&gt;

&lt;p&gt;但是，在 nonstationary problems  中并不适用，因为它鼓励 explore  的作用持续很短，只要$n$ 稍大些，初值的效果便会被消去
事实上，在一般的 nonstationary case 中，任何关于初始条件的方法均很难起到作用，包括 sample-average methods ，它也将初值视作一个特殊值，因为它对所有 rewards 的权重都是相等的。&lt;/p&gt;

&lt;h4 id=&quot;exercise-3&quot;&gt;Exercise：&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;2.6：Mysterious Spikes&lt;/em&gt;：为什么上图中，$Q_1 = 5$ 这条线的前期会有陡峭的振荡，如何优化？&lt;/p&gt;

&lt;p&gt;实验发现，陡峭的峰点位于$step=11$ 处，也就是说，在第11次 action 时，发生了高概率选中 optimal action 的情况；&lt;/p&gt;

&lt;p&gt;这是因为，在前面10(本例中，$k=10$)次选择中，没被选过的 action 的 $Q$ 值为 $5$ ，而所有被选中的 action 的 $Q$ 值都会被减小；所以前10次选择的结果都是遍历$k$个 action ，因此1-10次选中 optimal action 的概率都是 $10\%$；而到了第11次选择时，因为经过了一遍遍历， optimal action 的估计值会比 non-optimal action 的估计值要以较大的概率(约43%)高出一些，所以会增加中选概率;第12,13,14次选择都有类似的情况。而到了14以后， optimal action 的优势又被随机量给覆盖了(多次选择后， optimal action  突出的情况被抹消)，没有了第一次选择的大优势，曲线逐渐恢复平稳。&lt;/p&gt;

&lt;h3 id=&quot;27-upper-confidence-bound-action-selection&quot;&gt;2.7 Upper-Confidence-Bound Action Selection&lt;/h3&gt;
&lt;p&gt;之前的  explore  中，  ε-greedy methods  对于  non-greedy action  是随机选的。&lt;/p&gt;

&lt;p&gt;upper confidence bound (UCB)  会根据 non-greedy action  的 greedy  程度来选择：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_t\doteq \arg\max_a\Big[Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}\Big]&lt;/script&gt;

&lt;p&gt;$N_t(a)$ 是  action  $a$ 在 $t$ 时刻前被选过的次数，当其为 $0$ 时视 $a$ 为  greedy-action  ，$c &amp;gt; 0$ 控制  exploration  的强度，$c$ 越大， explore  的力度越大&lt;/p&gt;

&lt;p&gt;该方法中，平方根中的值表明了  value  估计值的不确定程度或方差；$\arg\max$ 相当于对  action value  的上界  upper bound  做一个排序，$c$ 表明了这个排序的可信度&lt;/p&gt;

&lt;p&gt;action  $a$ 被选中的次数越多，其不确定性就越小 (分母变小)；相对的，随着 $t$ 的增大，没被选中的那些  action  的不确定度会增大，使用对数表明时间越往后，不确定值的增长越小，但它是无界的&lt;/p&gt;

&lt;p&gt;在所有的  action  被选得多了后，那些 $Q$ 值小的、被选中次数多的  action ，会减少被选中的频率&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images//RL-Intro-Chapter2/upside-of-ucb.png&quot; alt=&quot;upside-of-ucb&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图，其中 $\epsilon = -1$ 的即是  UCB ，前面的抖动是在每轮遍历所有  action  后发生的：第一轮选取时，那些 $N_t(a) = 0$ 的视为  greedy-action  ，因此前 $k$ 次会遍历所有  action ，到了第 $k+1$ 次时，由于所有的 $N_t(a) = 1$，因此会选中 $Q_t(a)$ 最高的那个，于是大概率选中了 optimal action ；第二轮也是类似的情况，在式中不确定值起到较大的作用，到了都遍历两轮后，便是 $Q_t(a)$ 起到较大作用，多次如此后； $N_t(a)$ 变大，不确定度变小，于是曲线趋于平稳。&lt;/p&gt;

&lt;p&gt;UCB  虽然在本例中效果好于  ε-greedy  ，但是并不如  ε-greedy  实用，原因在于其更难扩展到其它普遍的RL 问题中(至少在本书后面的例子中是这样的);
一是不好处理  nonstationary problems ，二是不好处理较大的状态空间，尤其是在用函数逼近的情况下；在这些情况中  UCB  并不实用&lt;/p&gt;

&lt;h3 id=&quot;28-gradient-bandit-algorithms&quot;&gt;2.8 Gradient Bandit Algorithms&lt;/h3&gt;
&lt;p&gt;之前的方法都是估计一个  action values ，然后用它来选 actions ，本节会用一个 numerical preference — $H_t(a)$  来选action：&lt;/p&gt;

&lt;p&gt;$H_t(a)$ 越大，$a$ 被选中的概率越大，但是无法给出 $H_t(a)$ 与  reward  的关系表达&lt;/p&gt;

&lt;p&gt;$H_t(a)$ 的值中，只有不同的 $a$ 之间的差值是有意义的，因此，使用  soft-max distribution  来选择 $a$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mit {Pr}\{A_t=a\}\doteq \frac {e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}\doteq \pi_t(a),&lt;/script&gt;

&lt;p&gt;$\pi_t(a)$ 表示了在 $t$ 时刻选择  action $a$  的概率，对于任意的 $a$ ，有：$H_1(a)=0$&lt;/p&gt;

&lt;h4 id=&quot;exercise-4&quot;&gt;Exercise&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;2.7&lt;/em&gt;: 说明：在仅有两个  action  的情况下，使用  soft-max distribution  与使用  logistic(sigmoid) function  是一样的&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;**待做**
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在这种 &lt;strong&gt;随机梯度上升&lt;/strong&gt; (&lt;em&gt;stochastic gradient ascent&lt;/em&gt;) 的情形中，有一个很直觉的算法，根据每一步选中的  action  $A_t$ 和所获得的  reward  $R_t$ 更新所有 $H_{t+1}(a)$ :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
H_{t+1}(A_t)&amp;\doteq H_t(A_t)+\alpha(R_t-\bar R_t)\big(1-\pi_t(A_t)\big),\quad \text{and}  \\
H_{t+1}(a)&amp;\doteq H_t(a)-\alpha(R_t-\bar R_t)\pi_t(a), \qquad \qquad \quad \text{for all $a\neq A_t$},
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中， $\bar R_t\in \Bbb R$ 在 $t$ (包含)时刻以前，所有  rewards  的平均值，它作为一个  baseline  与其它  rewards  作比较。如果新的  reward  高于它，就提高 $A_t$ 的被选几率；反之则降低。
未选中的  actions  与被选中的  action  操作相反。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images//RL-Intro-Chapter2/with_out_baseline.png&quot; alt=&quot;with_out_baseline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接下来验证上述算法是&lt;strong&gt;梯度上升&lt;/strong&gt;的一种形式：&lt;/p&gt;

&lt;p&gt;标准&lt;strong&gt;梯度上升&lt;/strong&gt;的形式如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{t+1}(a)\doteq H_t(a)+\alpha \frac{\partial \Bbb E[R_t]}{\partial H_t(a)},&lt;/script&gt;

&lt;p&gt;其中 $\Bbb E[R_t]=\sum_x\pi_t(x)q_\ast (x)$ ,虽然 $q_\ast (x)$ 并不完全可知，但是其期望值与我们接下来在式中所用的是相等的&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac {\partial \Bbb E[R_t]}{\partial H_t(a)} &amp;= \frac {\partial}{\partial H_t(a)}\Big[\sum_x\pi_t(x)q_\ast (x)\Big] \\
&amp; = \sum_xq_\ast (x)\frac {\partial \pi_t(x)}{\partial H_t(a)} \\
&amp; = \sum_x\big(q_\ast (x)-B_t\big)\frac {\partial \pi_t(x)}{\partial H_t(a)}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中 $B_t$ 即为  baseline  ，其与 $x$ 无关。因为 $\sum_x\frac {\partial \pi_t(x)}{\partial H_t(a)}=0$ ，因此，加入 $B_t$ 并不会改变等式。&lt;/p&gt;

&lt;p&gt;接下来分式上下同乘 $\pi_t(x)$ ，就得到了期望的形式：&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac {\partial \Bbb E[R_t]}{\partial H_t(a)} &amp; = \sum_x\pi_t(x)\big(q_\ast (x)-B_t\big)
\frac {\partial \pi_t(x)}{\partial H_t(a)}/\pi_t(x) \\
&amp;= \Bbb E\bigg[\big(q_\ast (A_t)-B_t\big)\frac {\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)\bigg] \\
&amp;= \Bbb E\bigg[\big(R_t-\bar R_t\big)\frac {\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)\bigg] \\
&amp; = \Bbb E\Big[\big(R_t-\bar R_t\big)\pi_t(A_t)\big(\Bbb I_{a=A_t}-\pi_t(a)\big)/\pi_t(A_t)\Big] \qquad \text{(下面证明)}\\
&amp; = \Bbb E\Big[\big(R_t-\bar R_t\big)\big(\Bbb I_{a=A_t}-\pi_t(a)\big)\Big]
\end{align} %]]&gt;&lt;/script&gt;&lt;br /&gt;
令 $B_t = \bar R_t$ ，因为 $\Bbb E[R_t|A_t]=q_\ast (A_t)$，所以可以将 $q_\ast (A_t)$ 替换成 $R_t$ ， 就得到了算法中所用的更新公式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{t+1}(a)=H_t(a)+\alpha(R_t-\bar R_t)\big(\Bbb I_{a=A_t}-\pi_t(a)\big),\quad \text{for all $a$},&lt;/script&gt;

&lt;p&gt;下面证明 $\frac {\partial \pi_t(x)}{\partial H_t(a)}=\pi_t(x)\big(\Bbb I_{a=x}-\pi_t(a)\big)$ , 其中 $\Bbb I_{a=x}$ 在 $a=x$ 时为 $1$ ，在$a\neq x$ 时为 $0$ :&lt;/p&gt;

&lt;p&gt;首先引出分式的偏导数的标准形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {\partial}{\partial x}\bigg[\frac{f(x)}{g(x)}\bigg]
=\frac{\frac{\partial f(x)}{\partial x}g(x)-f(x)\frac{\partial g(x)}{\partial x}}
{g(x)^2}&lt;/script&gt;

&lt;p&gt;利用该式，做以下推导：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial \pi_t(x)}{\partial H_t(a)}&amp;=\frac{\partial }{\partial H_t(a)}\pi_t(x) \\
&amp;= \frac{\partial }{\partial H_t(a)}\bigg[\frac{e^{H_t(x)}}{\sum_{y=1}^ke^{H_t(y)}}\bigg] \\
&amp;= \frac{\frac{\partial e^{H_t(x)}}{\partial H_t(a)}\sum_{y=1}^ke^{H_t(y)}-e^{H_t(x)}\frac{\partial\sum_{y=1}^ke^{H_t(y)}}{\partial H_t(a)}}
{\Big(\sum_{y=1}^ke^{H_t(y)}\Big)^2} \quad &amp;\text{(分式偏导数)} \\
&amp;= \frac{\Bbb I_{a=x}e^{H_t(x)}\sum_{y=1}^ke^{H_t(y)}-e^{H_t(x)}e^{H_t(a)}}
{\Big(\sum_{y=1}^ke^{H_t(y)}\Big)^2} &amp; \big(\frac{\partial e^x}{\partial x}=e^x\big) \\
&amp;= \frac{\Bbb I_{a=x}e^{H_t(x)}}{\sum_{y=1}^ke^{H_t(y)}}-\frac{e^{H_t(x)}e^{H_t(a)}}{\Big(\sum_{y=1}^ke^{H_t(y)}\Big)^2} \\
&amp;= \Bbb I_{a=x}\pi_t(x)-\pi_t(x)\pi_t(a) \\
&amp;= \pi_t(x)\big(\Bbb I_{a=x}-\pi_t(a)\big).
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;随机梯度上升&lt;/strong&gt;能够保证算法具有鲁棒的收敛性&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：算法的更新不依赖于所选的动作  action  ，也不依赖于奖励基线  baseline  ， baseline  的取值不会影响更新的结果，不过会影响到更新收敛的速度。因为梯度的期望不受  baseline  的影响，但梯度的方差受到了影响。&lt;/p&gt;

&lt;h3 id=&quot;29-associative-search-contextual-bandits&quot;&gt;2.9 Associative Search (Contextual Bandits)&lt;/h3&gt;
&lt;p&gt;之前讨论的是  nonassociative tasks ，也即不需要建立状态与动作之间的联系，接下来将讨论  associative task ，需要建立从状态到最优动作之间的映射关系。&lt;/p&gt;

&lt;p&gt;举个例子：一个新的老虎机问题：现在我们需要在10个老虎机问题中中进行决策，这10组老虎机的 $q_\ast (a)$ 各不相同，每次随机选择一组老虎机进行选择，如果不知道或者不使用老虎机组合的编号，那么上面的方法将起不到任何作用。只有将老虎机组合的编号用上，为每组老虎机考虑不同的  action  ，才能得到理想的奖励。这就是为老虎机组合的状态(编号)与对应的动作之间建立起映射关系。&lt;/p&gt;

&lt;p&gt;Associative search taks  常被叫做  contextual bandits  ，其介于简单的  k-armed banditproblem  和完全的RL问题之间，它虽然建立了状态与动作的联系，但是动作还是只影响到立即回报，而不影响后续状态。&lt;/p&gt;

&lt;h4 id=&quot;exercise-5&quot;&gt;Exercise&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;2.8&lt;/em&gt;：如果不知道是哪一个  case  ，那么无论怎么选，动作 1 和动作 2 的期望回报都是 $0.5$ 。
而如果知道是哪一个  case  ，那么在学习到  case  与  action value  之间的映射后，就能总是选到  value  更高的  action  ，那么期望回报能够达到 $0.55$&lt;/p&gt;

&lt;h3 id=&quot;210-summary&quot;&gt;2.10 Summary&lt;/h3&gt;
&lt;p&gt;本章介绍了权衡  exploration  和  exploitation  的几个简单方法。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;$\epsilon$ methods&lt;/em&gt; 是以小概率随机选取  non-greedy action&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;UCB methods&lt;/em&gt; 则是会在  explore  时偏向那些样本较少的  action  。当然，这是以  value  为基准的&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Gradient&lt;/em&gt; 并不估计  action values  ，而是  action preferences  ，&lt;/li&gt;
  &lt;li&gt;简单的初始化可以让算法有效地进行  explore&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上方法在  10-armed testbed  中的性能表现如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images//RL-Intro-Chapter2/diff-parameter-method-perform.png&quot; alt=&quot;diff-parameter-method-perform&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这些方法的性能都受到参数的影响，我们在考虑方法的性能时，不仅要考虑其在最优参数处表现出的性能，还要考虑方法本身对参数的敏感性。如果方法足够敏感，调参会方便些，但如果太过敏感，也许又会使其失去泛化能力和可重复性。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在  k-armed bandit problems  中，平衡  exploration  和  exploitation  的最有方法是  Gittins indices  ，但它假设了可能问题的先验分布是已知的，而这无论在理论上还是在计算易处理性上都不能推广到完全的RL问题中。
*&lt;/li&gt;
  &lt;li&gt;贝叶斯方法假定了  action values  的一个已知的初始分布。一般来说，其更新的计算过程会非常复杂，除了某些特定的分布外( conjugate priors )，一个可能的办法是在每一步根据可能是最优动作的后验概率来选择  action  。该方法常被叫做  posterior sampling  或  Thompson sampling  。
*&lt;/li&gt;
  &lt;li&gt;贝叶斯方法可想见能够达到  exploration  和  exploitation  的最优平衡。能够为所有的动作计算可能得到的立即回报、导致的后验分布与动作值的关系。但是它的状态空间增长得太快，几乎不可能完成如此巨大的计算量，但是逼近它是有可能的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;exercise-6&quot;&gt;Exercise&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;2.9 (programming)&lt;/em&gt;: 给出一张类似图2.6的图，基于  Exercise 2.5  ，   non-stationary case  ，$\epsilon$-greedy
 method , $\alpha=0.1$ ，每轮 $200,000$ 步，对于每一种算法-参数组，使用后 $100,000$ 步的平均奖励作为数据。&lt;/p&gt;
</description>
        <pubDate>Sat, 18 May 2019 00:53:30 +0800</pubDate>
        <link>http://localhost:4000/2019/05/RL-Intro-Chapter2/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/05/RL-Intro-Chapter2/</guid>
        
        <category>强化学习</category>
        
        
      </item>
    
      <item>
        <title>CHAPTER 1. INTRODUCTION</title>
        <description>&lt;p&gt;RL是通过与环境进行交互行为，并通过计算来学习的一种方法；相比较其它机器学习方法，更关注于交互，直指实验目标。&lt;br /&gt;
&lt;strong&gt;RL要做的是&lt;/strong&gt;：学会做什么：即如何从当前situation来得到action，来最大化reward&lt;br /&gt;
learner需要通过不断的尝试来发现哪个action能够产生较大的reward&lt;br /&gt;
较复杂的情况：actions不仅影响reward，还会影响&lt;strong&gt;下一个situation&lt;/strong&gt;，并且由此影响到后续的rewards&lt;br /&gt;
RL的两个显著特征：&lt;em&gt;trial-and-error&lt;/em&gt;(摸石头过河)、&lt;em&gt;delayed reward&lt;/em&gt; (延后的反馈)&lt;br /&gt;
Reinforcement Learning 既能表示增强学习的问题，也能表示解决它的方法，还能表示研究这类问题及其解决办法的领域&lt;br /&gt;
需要注重区分这类表达方式，避免造成混淆，尤其要分清：增强学习问题和解决方法的区别&lt;br /&gt;
可以用动态系统-dynamic system 理论 (数学概念) 中的观点来描述增强学习 &lt;br /&gt;
具体来说就是：the optimal control of incompletely-known Markov decision processes.	(Chapter 3)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;一个学习代理必须具备三个条件：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;能够感知周围环境的状态 state&lt;/li&gt;
  &lt;li&gt;能够做出影响 state 的行为 action&lt;/li&gt;
  &lt;li&gt;一个与 state 相关的目标 goal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Markov decision processes 包含了这三个方面：sensation、action、goal&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RL与其它机器学习方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;监督学习：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;由具备知识的外部监督者 supervisor 提供标记过的数据集，用于学习&lt;/li&gt;
  &lt;li&gt;目标：在训练集以外的数据上，能够做出准确的判断&lt;/li&gt;
  &lt;li&gt;监督学习不适用于交互&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;非监督学习：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;寻找未标记数据中的隐含结构信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;强化学习&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;获得最大的奖励 reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RL的挑战：&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;trade-off between explration and exploitation&lt;/em&gt;&lt;br /&gt;
这个问题一直没有得到解决&lt;br /&gt;
RL还有一个特征，就是关注于在代理与环境的交互中直接解决目标问题&lt;br /&gt;
其它机器学习常常关注大环境中的子问题，尽管这同样出了许多喜人的成果，但这总归是一个很大的限制&lt;br /&gt;
RL则相反，代理总是会关注一个明确的目标，能够感知环境并做出影响环境的行为，环境通常具有很大的不确定性&lt;br /&gt;
当RL涉及规划时，它必须解决规划与实际行为对环境造成的影响之间产生的相互作用，即环境变化对原定规划的影响，以及规划中对环境变化的判断，同时环境本身模型的改进与调整也是一个问题&lt;br /&gt;
当RL涉及监督学习时，需要分离出一个子问题，这个子问题应当在代理中扮演一个明确的角色，哪怕其在完整代理中无法填充细节&lt;br /&gt;
RL的代理并不是单纯指向机器人一类的完整结构，也可以指向一个更大系统中的一部分，此时它与整个系统中的其它部分产生直接交互，与该系统所处环境产生间接交互&lt;br /&gt;
人们必须超越显而易见的代理人及其环境的例子来理解强化学习框架的普遍性&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RL的四个要素&lt;/strong&gt;：策略 &lt;em&gt;a policy&lt;/em&gt; 、奖励信号 &lt;em&gt;a reward signal&lt;/em&gt; 、价值函数 &lt;em&gt;a value function&lt;/em&gt; 、环境模型 &lt;em&gt;a model of the environment&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;policy：定义了在什么场景 state 下应当做出什么行为 action ，有时会是一个搜索的过程。一般而言，策略可能是随机的&lt;/li&gt;
  &lt;li&gt;reward signal：定义了RL的学习目标，决定了行为的好坏，是策略更新的基础。一般而言，reward signal 是state 和 action 的随机函数&lt;/li&gt;
  &lt;li&gt;value function：RL的长期目标，在某一个 state +action 下，其产生的 reward 可能是低的，但其后续造成的影响能带来更高的 value ，这是RL所需要的&lt;br /&gt;
  actions 的最终目标总是获取更高的values，但是 reward 在做出action 后可以由环境直接得出，而values 却需要估算且未必准确，有效估算values 的方法是RL的核心问题&lt;br /&gt;
  value 的另一个理解：为了保证获得更多的 total reward ，所追求的一个长期目标&lt;br /&gt;
  这里将 total reward 认为是RL的终极目标，而某些时候选择一些低 reward 的行为带来了更高的 value， 而高的 value 能够保证获得更高的total reward&lt;br /&gt;
  比如将 reward 视作人得到的愉悦，value 视作人生命的长度，那么高 value 可以保证得到更多的愉悦，但终极目标还是 total reward&lt;br /&gt;
  即在某些极高的 reward 面前，可以放弃 value ，就像人为了某些东西放弃生命一样，因为这可以带来极高的愉悦&lt;/li&gt;
  &lt;li&gt;model of the environment：在model-based RL 中会使用一个环境模型来预测环境的变化，这个模型不是真正的环境；而不使用环境模型，直接走一步看一步的RL方法称为 model-free 方法&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rl的局限与范围&quot;&gt;RL的局限与范围&lt;/h3&gt;

&lt;p&gt;RL强烈依赖于state ，state 可以理解为代理所在环境能够给出的任何信息，它告诉代理当前的环境是如何的
(state 本身非常重要，但本书更关注于如何根据state 给出action)&lt;br /&gt;
大部分RL方法围绕着estimating value function 来构造，然而estimating value function 并不是必须的&lt;br /&gt;
如遗传算法、遗传编程、模拟退火及一些其它优化方法就不去估计价值函数。&lt;br /&gt;
它们提供了多个统计策略，每个策略在一段长时间内各自与独立的环境实体交互，那些得到最高reward 的策略及其变种进入下一轮迭代&lt;br /&gt;
这种算法称为进化方法，当策略空间足够小，或者搜索时间足够长，进化算法能够取得有效的成果。&lt;br /&gt;
另外，进化算法在代理无法完全感知环境状态的问题中具有优势。&lt;br /&gt;
我们的重点是强化学习方法，这些方法在与环境相互作用时学习，而进化方法则不然&lt;br /&gt;
能够利用个体交互行为细节的方法在许多场合会比进化算法更有效。&lt;br /&gt;
进化算法忽略了许多RL问题的有用的结构：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;忽略了所搜索的策略是一个从states 到 actions 的函数&lt;/li&gt;
  &lt;li&gt;忽略了个体在其生命周期中传递的具体states 或 选择的具体 actions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;exampletic-tac-toe&quot;&gt;Example：Tic-Tac-Toe&lt;/h3&gt;

&lt;p&gt;假设对手有可能失误，要求找到对手的失误，使获胜的机会最大&lt;br /&gt;
虽然这个问题很简单，但是使用传统的方法并不容易得到满意的结果&lt;br /&gt;
minimax：会避免失败，但也会错失获胜机会&lt;br /&gt;
顺序决策问题的经典优化：如动态规划，可以为任何对手计算最优解，但需要输入该对手的完整规范，包括对手在每个棋盘状态下进行每次移动的概率。&lt;br /&gt;
大部分情况下，环境的完整信息无法提供。关于在这个问题上可以做的最好的事情是首先学习对手行为的模型，达到某种程度的置信度，然后应用动态规划来计算给定近似对手模型的最优解。&lt;br /&gt;
进化算法在该问题上会直接搜索可能的策略空间，找到一个获胜概率高的策略。这里的策略就是告诉player 对应每个游戏中的 state ，下一步应该写在哪里&lt;br /&gt;
对每个策略，都能够在与对手下棋的过程中获得一个对获胜概率的估计，这个估计能够在之后指导选择哪些策略&lt;br /&gt;
一个典型的进化算法就像在策略空间中爬山，在努力获得提升的过程中产生并估计策略，或者说，一个遗传类算法能够用于保持和估计一系列策略&lt;br /&gt;
使用value function：&lt;br /&gt;
首先为游戏中的所有可能的state 设置一组数表，每个数表示从对应状态获取胜利的可能性的最新估计，把这些估计座位state 的 value，整个数表就是学习到的value function&lt;br /&gt;
value 值高表明该状态获胜的机会大，己方已经三连的状态value 为1，对方已经达成三连子的状态value 则为0，其它状态的初始value 设为0.5&lt;br /&gt;
与对手进行对战，在落子时，检查每一个可能的落点对应的 states 并查表得到对应 value&lt;br /&gt;
大多数时候遵循贪婪法则，即总是选择胜率最高的下法，这就是 exploit ；偶尔地，在不是最高胜率的下法中随机选择一种，以获取新知识。这是 explore&lt;br /&gt;
在对战落子时，每次会更新state 对应的value ，尽量做到精确估计获胜概率。&lt;br /&gt;
为此，在每次greedy move 后将value 回溯到之前的 move ，更准确地说，早期state 的value 会被更新到近于后期state 的value ，通过将后期state 的value 的一小部分加到前期state 的 value 来完成&lt;br /&gt;
用 $s$ 表示greedy move 前的 state，用 $s’$ 表示 greedy move 后的 state，$V(s)$ 表示 $s$ 对应的 value ，那么：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(s)\leftarrow V(s)+\alpha[ V(s')-V(s)]&lt;/script&gt;

&lt;p&gt;α is a small positive fraction，称为 step-size parameter，影响学习的速率，这种方法叫做 &lt;em&gt;temporal-difference learning method&lt;/em&gt;&lt;br /&gt;
当 α 随时间适当下降时，该方法收敛；若 α 不随时间下降，策略也会缓慢改变下法，以更好应对当前对手&lt;br /&gt;
进化算法使用一个固定策略进行多次对局，然后按照获胜次数来判断该策略的优劣，但忽略了对局过程中的每一步所包含的信息；&lt;br /&gt;
使用value function ：则会在对局中，根据每一步造成的胜率变化来判断对应策略，比进化算法更加细化&lt;br /&gt;
两者都是在策略空间中进行搜索，但学习value function 在信息利用上更有优势&lt;/p&gt;

&lt;h4 id=&quot;exercise&quot;&gt;Exercise：&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;1.1：self-play&lt;/em&gt;：最后会变成进化算法，因为两者都会根据对方的下法调整自己的策略，也就是对手固定的假设不存在，那么value function最终会寻找一个不针对任何对手的策略&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1.2：Symmetries&lt;/em&gt;：可以用对称矩阵，加快学习速度，并不会改变其学习结果。如果对手不利用对称性，那么其在对称布局上的下法有可能不同，策略就不应该利用对称性来改变下法，还是应该对战中学到的策略来做应对，那么对称的state 就不会拥有对称的value，因为对手的下法是不同的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1.3：Greedy Play&lt;/em&gt;：如果总是Greedy Play，那么策略就不会学到新知识，就可能错过其它胜率更高的下法，就不会变得更好，也不会变得更差，当然，一旦对手开始变化，那么策略就有可能变差&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1.4：Learning from Exploration&lt;/em&gt;：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;With the step size parameter appropriately reduced, and assuming the exploration rate is fixed, the probability set with no learning from exploration is the value of each state given the optimal action from then on is taken, whereas with learning from exploration it is the expected value of each state including the active exploration policy. Using the former is better to learn, as it reduces variance from sub-optimal future states (e.g. if you can win a game of chess in one move, but if you perform another move your opponent wins, that doesn’t make it a bad state) The former would result in more wins all other things being equal.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 18 May 2019 00:53:30 +0800</pubDate>
        <link>http://localhost:4000/2019/05/RL-Intro-Chapter1/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/05/RL-Intro-Chapter1/</guid>
        
        <category>强化学习</category>
        
        
      </item>
    
  </channel>
</rss>
